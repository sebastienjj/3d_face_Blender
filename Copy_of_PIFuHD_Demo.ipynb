{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PIFuHD Demo",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sebastienjj/3d_face_Blender/blob/main/Copy_of_PIFuHD_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eclLG4xlJRIE"
      },
      "source": [
        "# PIFuHD Demo: https://shunsukesaito.github.io/PIFuHD/\n",
        "\n",
        "![](https://shunsukesaito.github.io/PIFuHD/resources/images/pifuhd.gif)\n",
        "\n",
        "Made by [![Follow](https://img.shields.io/twitter/follow/psyth91?style=social)](https://twitter.com/psyth91)\n",
        "\n",
        "To see how the model works, visit the project repository.\n",
        "\n",
        "[![GitHub stars](https://img.shields.io/github/stars/facebookresearch/pifuhd?style=social)](https://github.com/facebookresearch/pifuhd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmFdsTvLKtBO"
      },
      "source": [
        "## Note\n",
        "Make sure that your runtime type is 'Python 3 with GPU acceleration'. To do so, go to Edit > Notebook settings > Hardware Accelerator > Select \"GPU\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TfPAtL4CyZw"
      },
      "source": [
        "## More Info\n",
        "- Paper: https://arxiv.org/pdf/2004.00452.pdf\n",
        "- Repo: https://github.com/facebookresearch/pifuhd\n",
        "- Project Page: https://shunsukesaito.github.io/PIFuHD/\n",
        "- 1-minute/5-minute Presentation (see below)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DDpqpf2BABR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "outputId": "679345c3-b5b3-481a-f293-ea904727c183"
      },
      "source": [
        "import IPython\n",
        "IPython.display.HTML('<h2>1-Minute Presentation</h2><iframe width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/-1XYTmm8HhE\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe><br><h2>5-Minute Presentation</h2><iframe width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/uEDqCxvF5yc\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h2>1-Minute Presentation</h2><iframe width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/-1XYTmm8HhE\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe><br><h2>5-Minute Presentation</h2><iframe width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/uEDqCxvF5yc\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vZaAyhUJ9QC"
      },
      "source": [
        "## Requirements\n",
        "- Python 3\n",
        "- PyTorch tested on 1.4.0\n",
        "- json\n",
        "- PIL\n",
        "- skimage\n",
        "- tqdm\n",
        "- numpy\n",
        "- cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfPDep8LlP_I"
      },
      "source": [
        "## Help! I'm new to Google Colab\n",
        "\n",
        "You can check out the following youtube video on how to upload your own picture and run PIFuHD. **Note that with new update, you can upload your own picture more easily with GUI down below.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaMP1EitljaA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "outputId": "bd90dfb6-f34f-49eb-9986-52772844d82b"
      },
      "source": [
        "import IPython\n",
        "IPython.display.HTML('<iframe width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/LWDGR5v3-3o\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<iframe width=\"720\" height=\"405\" src=\"https://www.youtube.com/embed/LWDGR5v3-3o\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYhlsDkg1Hwb"
      },
      "source": [
        "## Clone PIFuHD repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmpEwdOd1G1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec8d1967-43da-432f-c232-e337dfe23255"
      },
      "source": [
        "!git clone https://github.com/facebookresearch/pifuhd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pifuhd'...\n",
            "remote: Enumerating objects: 213, done.\u001b[K\n",
            "remote: Total 213 (delta 0), reused 0 (delta 0), pack-reused 213\u001b[K\n",
            "Receiving objects: 100% (213/213), 402.76 KiB | 3.33 MiB/s, done.\n",
            "Resolving deltas: 100% (104/104), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvQm-A8ESKb2"
      },
      "source": [
        "## Configure input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvle9T10fB6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82f0e0bd-c8a5-4b28-be28-dc36a45058f8"
      },
      "source": [
        "cd /content/pifuhd/sample_images"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pifuhd/sample_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SI7Ye1JfIim"
      },
      "source": [
        "**If you want to upload your own picture, run the next cell**. Otherwise, go to the next next cell. Currently PNG, JPEG files are supported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaV_7Yi8fM-B",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "fe4c8404-44f0-43ae-df8a-02ee3bfc5363"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "filename = list(files.upload().keys())[0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f256c470-8a69-4c7b-a1b2-f7177cbe3c81\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f256c470-8a69-4c7b-a1b2-f7177cbe3c81\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-57e3bcd473cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m   \"\"\"\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    121\u001b[0m   result = _output.eval_js(\n\u001b[1;32m    122\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m--> 123\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m    124\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEzmmB01SOZp"
      },
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "  image_path = '/content/pifuhd/sample_images/%s' % filename\n",
        "except:\n",
        "  image_path = '/content/pifuhd/sample_images/babas_highres.png' # example image\n",
        "image_dir = os.path.dirname(image_path)\n",
        "file_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "\n",
        "# output pathes\n",
        "obj_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.obj' % file_name\n",
        "out_img_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.png' % file_name\n",
        "video_path = '/content/pifuhd/results/pifuhd_final/recon/result_%s_256.mp4' % file_name\n",
        "video_display_path = '/content/pifuhd/results/pifuhd_final/result_%s_256_display.mp4' % file_name"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "896EC7iQfXkj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3478bc2b-0bb8-4264-e233-e7a2b3481b12"
      },
      "source": [
        "cd /content"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbVmda9J5TDL"
      },
      "source": [
        "## Preprocess (for cropping image)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtMjWGNU5STe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af61f636-00a4-4c6c-fc44-ef905a4e3873"
      },
      "source": [
        "!git clone https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lightweight-human-pose-estimation.pytorch'...\n",
            "remote: Enumerating objects: 120, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 120 (delta 1), reused 1 (delta 0), pack-reused 115\u001b[K\n",
            "Receiving objects: 100% (120/120), 227.79 KiB | 2.28 MiB/s, done.\n",
            "Resolving deltas: 100% (50/50), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-vYklhI5dab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24d44259-209d-42c2-e6cd-168b7b941de1"
      },
      "source": [
        "cd /content/lightweight-human-pose-estimation.pytorch/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/lightweight-human-pose-estimation.pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRod9SOu77I6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3299ac17-ff18-4245-c8d8-1382075b9c1e"
      },
      "source": [
        "!wget https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-22 15:08:11--  https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\n",
            "Resolving download.01.org (download.01.org)... 23.7.125.32, 2600:1407:3c00:1487::4b21, 2600:1407:3c00:1480::4b21\n",
            "Connecting to download.01.org (download.01.org)|23.7.125.32|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 87959810 (84M) [application/octet-stream]\n",
            "Saving to: ‘checkpoint_iter_370000.pth’\n",
            "\n",
            "checkpoint_iter_370 100%[===================>]  83.88M  90.9MB/s    in 0.9s    \n",
            "\n",
            "2022-04-22 15:08:12 (90.9 MB/s) - ‘checkpoint_iter_370000.pth’ saved [87959810/87959810]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdRcDXe38lHB"
      },
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
        "from modules.keypoints import extract_keypoints, group_keypoints\n",
        "from modules.load_state import load_state\n",
        "from modules.pose import Pose, track_poses\n",
        "import demo\n",
        "\n",
        "def get_rect(net, images, height_size):\n",
        "    net = net.eval()\n",
        "\n",
        "    stride = 8\n",
        "    upsample_ratio = 4\n",
        "    num_keypoints = Pose.num_kpts\n",
        "    previous_poses = []\n",
        "    delay = 33\n",
        "    for image in images:\n",
        "        rect_path = image.replace('.%s' % (image.split('.')[-1]), '_rect.txt')\n",
        "        img = cv2.imread(image, cv2.IMREAD_COLOR)\n",
        "        orig_img = img.copy()\n",
        "        orig_img = img.copy()\n",
        "        heatmaps, pafs, scale, pad = demo.infer_fast(net, img, height_size, stride, upsample_ratio, cpu=False)\n",
        "\n",
        "        total_keypoints_num = 0\n",
        "        all_keypoints_by_type = []\n",
        "        for kpt_idx in range(num_keypoints):  # 19th for bg\n",
        "            total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n",
        "\n",
        "        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs)\n",
        "        for kpt_id in range(all_keypoints.shape[0]):\n",
        "            all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n",
        "            all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n",
        "        current_poses = []\n",
        "\n",
        "        rects = []\n",
        "        for n in range(len(pose_entries)):\n",
        "            if len(pose_entries[n]) == 0:\n",
        "                continue\n",
        "            pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n",
        "            valid_keypoints = []\n",
        "            for kpt_id in range(num_keypoints):\n",
        "                if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n",
        "                    pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n",
        "                    pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n",
        "                    valid_keypoints.append([pose_keypoints[kpt_id, 0], pose_keypoints[kpt_id, 1]])\n",
        "            valid_keypoints = np.array(valid_keypoints)\n",
        "            \n",
        "            if pose_entries[n][10] != -1.0 or pose_entries[n][13] != -1.0:\n",
        "              pmin = valid_keypoints.min(0)\n",
        "              pmax = valid_keypoints.max(0)\n",
        "\n",
        "              center = (0.5 * (pmax[:2] + pmin[:2])).astype(np.int)\n",
        "              radius = int(0.65 * max(pmax[0]-pmin[0], pmax[1]-pmin[1]))\n",
        "            elif pose_entries[n][10] == -1.0 and pose_entries[n][13] == -1.0 and pose_entries[n][8] != -1.0 and pose_entries[n][11] != -1.0:\n",
        "              # if leg is missing, use pelvis to get cropping\n",
        "              center = (0.5 * (pose_keypoints[8] + pose_keypoints[11])).astype(np.int)\n",
        "              radius = int(1.45*np.sqrt(((center[None,:] - valid_keypoints)**2).sum(1)).max(0))\n",
        "              center[1] += int(0.05*radius)\n",
        "            else:\n",
        "              center = np.array([img.shape[1]//2,img.shape[0]//2])\n",
        "              radius = max(img.shape[1]//2,img.shape[0]//2)\n",
        "\n",
        "            x1 = center[0] - radius\n",
        "            y1 = center[1] - radius\n",
        "\n",
        "            rects.append([x1, y1, 2*radius, 2*radius])\n",
        "\n",
        "        np.savetxt(rect_path, np.array(rects), fmt='%d')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6cGZD6f6IaY"
      },
      "source": [
        "net = PoseEstimationWithMobileNet()\n",
        "checkpoint = torch.load('checkpoint_iter_370000.pth', map_location='cpu')\n",
        "load_state(net, checkpoint)\n",
        "\n",
        "get_rect(net.cuda(), [image_path], 512)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0rgMInwTt0s"
      },
      "source": [
        "## Download the Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrIcZweSNRFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7005afd9-dcd7-4bb0-e126-fff6ea2ad00b"
      },
      "source": [
        "cd /content/pifuhd/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pifuhd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3jjm6HuQRk8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a395488-ff22-40a3-d314-2eaac6644bea"
      },
      "source": [
        "!sh ./scripts/download_trained_model.sh"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+ mkdir -p checkpoints\n",
            "+ cd checkpoints\n",
            "+ wget https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt pifuhd.pt\n",
            "--2022-04-22 15:08:29--  https://dl.fbaipublicfiles.com/pifuhd/checkpoints/pifuhd.pt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.75.142, 172.67.9.4, 104.22.74.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.75.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1548375177 (1.4G) [application/octet-stream]\n",
            "Saving to: ‘pifuhd.pt’\n",
            "\n",
            "pifuhd.pt           100%[===================>]   1.44G  55.4MB/s    in 23s     \n",
            "\n",
            "2022-04-22 15:08:52 (64.5 MB/s) - ‘pifuhd.pt’ saved [1548375177/1548375177]\n",
            "\n",
            "--2022-04-22 15:08:52--  http://pifuhd.pt/\n",
            "Resolving pifuhd.pt (pifuhd.pt)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘pifuhd.pt’\n",
            "FINISHED --2022-04-22 15:08:52--\n",
            "Total wall clock time: 23s\n",
            "Downloaded: 1 files, 1.4G in 23s (64.5 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6heKcA-0QEBw"
      },
      "source": [
        "## Run PIFuHD!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5995t2PnQTmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ded246d-e760-47e5-92da-59d4ce39efab"
      },
      "source": [
        "# Warning: all images with the corresponding rectangle files under -i will be processed. \n",
        "!python -m apps.simple_test -r 256 --use_rect -i $image_dir\n",
        "\n",
        "# seems that 256 is the maximum resolution that can fit into Google Colab. \n",
        "# If you want to reconstruct a higher-resolution mesh, please try with your own machine. "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming from  ./checkpoints/pifuhd.pt\n",
            "Warning: opt is overwritten.\n",
            "test data size:  1\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "generate mesh (test) ...\n",
            "  0% 0/1 [00:00<?, ?it/s]/content/pifuhd/lib/data/EvalDataset.py:87: UserWarning: loadtxt: Empty input file: \"/content/pifuhd/sample_images/babas_highres_rect.txt\"\n",
            "  rects = np.loadtxt(rect_path, dtype=np.int32)\n",
            "  0% 0/1 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/pifuhd/apps/simple_test.py\", line 30, in <module>\n",
            "    reconWrapper(cmd, args.use_rect)\n",
            "  File \"/content/pifuhd/apps/recon.py\", line 220, in reconWrapper\n",
            "    recon(opt, use_rect)\n",
            "  File \"/content/pifuhd/apps/recon.py\", line 205, in recon\n",
            "    test_data = test_dataset[i]\n",
            "  File \"/content/pifuhd/lib/data/EvalDataset.py\", line 131, in __getitem__\n",
            "    return self.get_item(index)\n",
            "  File \"/content/pifuhd/lib/data/EvalDataset.py\", line 93, in get_item\n",
            "    im = crop_image(im, rect)\n",
            "  File \"/content/pifuhd/lib/data/EvalDataset.py\", line 17, in crop_image\n",
            "    x, y, w, h = rect\n",
            "ValueError: not enough values to unpack (expected 4, got 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUZ8Nt5rNFXZ"
      },
      "source": [
        "## Render the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xp5s5uiOiDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0afd0c2c-5bbd-4537-f9bb-1307a8f2ae30"
      },
      "source": [
        "!pip install pytorch3d\n",
        "\n",
        "# If you get an error in the next cell, you can instead try the following command (don't forget to comment out the one above!).\n",
        "# Note that this error is caused by inconsistent cuda version for the pytorch3d package and pytorch in Colab environment.\n",
        "# Thus, this issue may persist unless pytorch3d in the pip package is updated with the cuda version consistent with pytorch in Colab.\n",
        "# Also please be aware that the following command is much slower as it builds pytorch3d from scratch.\n",
        "\n",
        "# !pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\n",
        "\n",
        "# You can try another solution below as well. This is also slow and requires you to restart the runtime.\n",
        "\n",
        "# !pip install 'torch==1.6.0+cu101' -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install 'torchvision==0.7.0+cu101' -f https://download.pytorch.org/whl/torch_stable.html\n",
        "# !pip install 'pytorch3d==0.2.5'"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch3d\n",
            "  Downloading pytorch3d-0.3.0-cp37-cp37m-manylinux1_x86_64.whl (30.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 30.0 MB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.4 in /usr/local/lib/python3.7/dist-packages (from pytorch3d) (0.11.1+cu111)\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20220414.tar.gz (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4->pytorch3d) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4->pytorch3d) (7.1.2)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4->pytorch3d) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision>=0.4->pytorch3d) (4.1.1)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 36.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorch3d) (4.64.0)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorch3d) (1.1.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fvcore->pytorch3d) (0.8.9)\n",
            "Collecting iopath>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: fvcore\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20220414-py3-none-any.whl size=61211 sha256=a68635e1848b60c83ab44ec93732aba06611a69a422a888deccb76f8aa9e34a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/f4/b8/7b5df8b6722f4c72315ce70c31aa693e00cef6a5056149bd28\n",
            "Successfully built fvcore\n",
            "Installing collected packages: pyyaml, portalocker, yacs, iopath, fvcore, pytorch3d\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed fvcore-0.1.5.post20220414 iopath-0.1.9 portalocker-2.4.0 pytorch3d-0.3.0 pyyaml-6.0 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afwL_-ROCmDf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "fa363259-a90e-4ee4-ef4b-73782f3221e3"
      },
      "source": [
        "from lib.colab_util import generate_video_from_obj, set_renderer, video\n",
        "\n",
        "renderer = set_renderer()\n",
        "generate_video_from_obj(obj_path, out_img_path, video_path, renderer)\n",
        "\n",
        "# we cannot play a mp4 video generated by cv2\n",
        "!ffmpeg -i $video_path -vcodec libx264 $video_display_path -y -loglevel quiet\n",
        "video(video_display_path)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-22614b7ef849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerate_video_from_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset_renderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgenerate_video_from_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_img_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/pifuhd/lib/colab_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Util function for loading meshes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_objs_as_meshes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch3d/io/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mobj_io\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_objs_as_meshes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_obj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mply_io\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_ply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_ply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch3d/io/obj_io.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmtl_io\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_mtl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_mesh_texture_atlas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_check_faces_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_make_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTexturesAtlas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTexturesUV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMeshes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_meshes_as_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch3d/renderer/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Copyright (c) Facebook, Inc. and its affiliates. All rights reserved.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m from .blending import (\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mBlendParams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mhard_rgb_blend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch3d/renderer/blending.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# pyre-fixme[21]: Could not find name `_C` in `pytorch3d`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpytorch3d\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: /usr/local/lib/python3.7/dist-packages/pytorch3d/_C.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZNK2at6Tensor7is_cudaEv",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUEXAvcvkVYV"
      },
      "source": [
        "## Tips for Inputs: My results are broken!\n",
        "\n",
        "(Kudos to those who share results on twitter with [#pifuhd](https://twitter.com/search?q=%23pifuhd&src=recent_search_click&f=live) tag!!!!)\n",
        "\n",
        "Due to the limited variation in the training data, your results might be broken sometimes. Here I share some useful tips to get resonable results. \n",
        "\n",
        "*   Use high-res image. The model is trained with 1024x1024 images. Use at least 512x512 with fine-details. Low-res images and JPEG artifacts may result in unsatisfactory results. \n",
        "*   Use an image with a single person. If the image contain multiple people, reconstruction quality is likely degraded.\n",
        "*   Front facing with standing works best (or with fashion pose)\n",
        "*   The entire body is covered within the image. (Note: now missing legs is partially supported)\n",
        "*   Make sure the input image is well lit. Exteremy dark or bright image and strong shadow often create artifacts.\n",
        "*   I recommend nearly parallel camera angle to the ground. High camera height may result in distorted legs or high heels. \n",
        "*   If the background is cluttered, use less complex background or try removing it using https://www.remove.bg/ before processing.\n",
        "*   It's trained with human only. Anime characters may not work well (To my surprise, indeed many people tried it!!).\n",
        "*   Search on twitter with [#pifuhd](https://twitter.com/search?q=%23pifuhd&src=recent_search_click&f=live) tag to get a better sense of what succeeds and what fails. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6U0K5CNAO_u"
      },
      "source": [
        "## Share your result! \n",
        "Please share your results with[ #pifuhd](https://twitter.com/search?q=%23pifuhd&src=recent_search_click&f=live) tag on Twitter. Sharing your good/bad results helps and encourages the authors to further push towards producition-quality human digitization at home.\n",
        "**As the tweet buttom below doesn't add the result video automatically, please download the result video above and manually add it to the tweet.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CBxbdrM9F-9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "outputId": "26d49b7c-ba5d-44f2-a3db-dd1978baf25d"
      },
      "source": [
        "import IPython\n",
        "IPython.display.HTML('<a href=\"https://twitter.com/intent/tweet?button_hashtag=pifuhd&ref_src=twsrc%5Etfw\" class=\"twitter-hashtag-button\" data-size=\"large\" data-text=\"Google Colab Link: \" data-url=\"https://bit.ly/37sfogZ\" data-show-count=\"false\">Tweet #pifuhd</a><script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  (Don\\'t forget to add your result to the tweet!)')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<a href=\"https://twitter.com/intent/tweet?button_hashtag=pifuhd&ref_src=twsrc%5Etfw\" class=\"twitter-hashtag-button\" data-size=\"large\" data-text=\"Google Colab Link: \" data-url=\"https://bit.ly/37sfogZ\" data-show-count=\"false\">Tweet #pifuhd</a><script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>  (Don't forget to add your result to the tweet!)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d-1pR8UR7PR"
      },
      "source": [
        "## Cool Applications\n",
        "Special thanks to those who play with PIFuHD and came up with many creative applications!! If you made any cool applications, please tweet your demo with [#pifuhd](https://twitter.com/search?q=%23pifuhd&src=recent_search_click&f=live). I'm constantly checking results there.\n",
        "If you need complete texture on the mesh, please try my previous work [PIFu](https://github.com/shunsukesaito/PIFu) as well! It supports 3D reconstruction + texturing from a single image although the geometry quality may not be as good as PIFuHD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68JDAYJFSFMV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a991cb50-e78c-42e4-a932-b719ea320ad7"
      },
      "source": [
        "IPython.display.HTML('<h2>Rigging (Mixamo) + Photoreal Rendering (Blender)</h2><blockquote class=\"twitter-tweet\"><p lang=\"pt\" dir=\"ltr\">vcs ainda tem a PACHORRA de me dizer que eu não sei dançar<a href=\"https://twitter.com/hashtag/b3d?src=hash&amp;ref_src=twsrc%5Etfw\">#b3d</a> <a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> <a href=\"https://t.co/kHCnLh6zxH\">pic.twitter.com/kHCnLh6zxH</a></p>&mdash; lukas arendero (@lukazvd) <a href=\"https://twitter.com/lukazvd/status/1274810484798128131?ref_src=twsrc%5Etfw\">June 21, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>FaceApp + Rigging (Mixamo)</h2><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\">カツラかぶってる自分に見える <a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> <a href=\"https://t.co/V8o7VduTiG\">pic.twitter.com/V8o7VduTiG</a></p>&mdash; Shuhei Tsuchida (@shuhei2306) <a href=\"https://twitter.com/shuhei2306/status/1274507242910314498?ref_src=twsrc%5Etfw\">June 21, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>Rigging (Mixamo) + AR (Adobe Aero)</AR><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\">写真→PIFuHD→Mixamo→AdobeAeroでサウンド付きARを作成。Zip化してLINEでARコンテンツを共有。<br>写真が1枚あれば簡単にARの3Dアニメーションが作れる時代…凄い。<a href=\"https://twitter.com/hashtag/PIFuHD?src=hash&amp;ref_src=twsrc%5Etfw\">#PIFuHD</a> <a href=\"https://twitter.com/hashtag/AdobeAero?src=hash&amp;ref_src=twsrc%5Etfw\">#AdobeAero</a> <a href=\"https://twitter.com/hashtag/Mixamo?src=hash&amp;ref_src=twsrc%5Etfw\">#Mixamo</a> <a href=\"https://t.co/CbiMi4gZ0K\">pic.twitter.com/CbiMi4gZ0K</a></p>&mdash; モジョン (@mojon1) <a href=\"https://twitter.com/mojon1/status/1273217947872317441?ref_src=twsrc%5Etfw\">June 17, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>3D Printing</h2><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\"><a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> 楽しい〜<br>小さい自分プリントした <a href=\"https://t.co/4qyWuij0Hs\">pic.twitter.com/4qyWuij0Hs</a></p>&mdash; isb (@vxzxzxzxv) <a href=\"https://twitter.com/vxzxzxzxv/status/1273136266406694913?ref_src=twsrc%5Etfw\">June 17, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h2>Rigging (Mixamo) + Photoreal Rendering (Blender)</h2><blockquote class=\"twitter-tweet\"><p lang=\"pt\" dir=\"ltr\">vcs ainda tem a PACHORRA de me dizer que eu não sei dançar<a href=\"https://twitter.com/hashtag/b3d?src=hash&amp;ref_src=twsrc%5Etfw\">#b3d</a> <a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> <a href=\"https://t.co/kHCnLh6zxH\">pic.twitter.com/kHCnLh6zxH</a></p>&mdash; lukas arendero (@lukazvd) <a href=\"https://twitter.com/lukazvd/status/1274810484798128131?ref_src=twsrc%5Etfw\">June 21, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>FaceApp + Rigging (Mixamo)</h2><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\">カツラかぶってる自分に見える <a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> <a href=\"https://t.co/V8o7VduTiG\">pic.twitter.com/V8o7VduTiG</a></p>&mdash; Shuhei Tsuchida (@shuhei2306) <a href=\"https://twitter.com/shuhei2306/status/1274507242910314498?ref_src=twsrc%5Etfw\">June 21, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>Rigging (Mixamo) + AR (Adobe Aero)</AR><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\">写真→PIFuHD→Mixamo→AdobeAeroでサウンド付きARを作成。Zip化してLINEでARコンテンツを共有。<br>写真が1枚あれば簡単にARの3Dアニメーションが作れる時代…凄い。<a href=\"https://twitter.com/hashtag/PIFuHD?src=hash&amp;ref_src=twsrc%5Etfw\">#PIFuHD</a> <a href=\"https://twitter.com/hashtag/AdobeAero?src=hash&amp;ref_src=twsrc%5Etfw\">#AdobeAero</a> <a href=\"https://twitter.com/hashtag/Mixamo?src=hash&amp;ref_src=twsrc%5Etfw\">#Mixamo</a> <a href=\"https://t.co/CbiMi4gZ0K\">pic.twitter.com/CbiMi4gZ0K</a></p>&mdash; モジョン (@mojon1) <a href=\"https://twitter.com/mojon1/status/1273217947872317441?ref_src=twsrc%5Etfw\">June 17, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script><h2>3D Printing</h2><blockquote class=\"twitter-tweet\"><p lang=\"ja\" dir=\"ltr\"><a href=\"https://twitter.com/hashtag/pifuhd?src=hash&amp;ref_src=twsrc%5Etfw\">#pifuhd</a> 楽しい〜<br>小さい自分プリントした <a href=\"https://t.co/4qyWuij0Hs\">pic.twitter.com/4qyWuij0Hs</a></p>&mdash; isb (@vxzxzxzxv) <a href=\"https://twitter.com/vxzxzxzxv/status/1273136266406694913?ref_src=twsrc%5Etfw\">June 17, 2020</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX5CTTW_KWhQ"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}